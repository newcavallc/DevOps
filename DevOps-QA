

WHAT IS DEVOPS?
DevOps is a culture that enhances automation, quality and speed in software delivery through 
the collaboration of Development and Operations. 

K8s:
aws-iam-authenticator: is a tool that authenticates users and services using AWS IAM credentials, also provides 
integration beteween IAM and K8s RBAC


1.) EC2 Instance Running Out of Disk Space:
-------------------------------------------
    Check /root - os:
           - Investigate and clear space in /root.
    Check the logs:
           - Examine and free up space in /var/log and /tmp.
    Check /application - for our application:
           - Inquire about the application's purpose and log location.
           - Explore clearing space in the application directory.
           - If necessary, take an EBS Snapshot and increase the associated volume size.


2.) Prometheus obtains metrics in two ways:
-------------------------------------------
    Pull Approach: Exposes metric endpoints (e.g., Java App/Node.js) for Prometheus to scrape data.

    Push Approach: The App sends metrics via a gateway back to Prometheus for storage.

    Q: Why choose PUSH?
    A: For apps that may not run continuously, PUSH ensures timely metric updates.

Prometheus is a toolkit for monitoring and alerting, ensuring system and application reliability. 
It collects metrics to inform decision-making, supporting dashboards and alerting. It monitors Docker 
containers, capturing CPU usage, memory, network traffic, and more, with built-in support for exporters 
like Node Exporter and Docker.


3.)What is Kubernetes kOpt (Kubernetes Operations)?
---------------------------------------------------
Kubernetes kOps is not limited to testing; it is a versatile tool used for production deployment and 
management of Kubernetes clusters.


4.) What is an instance fleet in AWS?
-------------------------------------
An AWS instance fleet is a diverse set of EC2 instances designed for specific use cases:
    * pricing models
    * Amazon EMR (Cloud-based big data platform) 
    * Application that require tailoring.


5.) Are you supporting any applications? If yes what application sever?
-----------------------------------------------------------------------
No, I haven't worked with application servers for:

Java: (Tomcat, JBoss, Weblogic)
NodeJS: (IIS)
Python: (Django)


6.) How to check logs on a Docker container - the last 200 lines?
-----------------------------------------------------------------
To check logs and filter only the last 200 lines, use the following command:
# docker container logs <container_name> --tail 200

This command will display the last 200 lines of the container logs.


7.)  What will happen to container logs if you restart the container? Will it be lost?
--------------------------------------------------------------------------------------
Restarting a container won't result in log loss; data is only lost upon container deletion 
due to their statelessness.

Consider using external persistent storage like SPLUNK for app logs. SPLUNK is effective for 
log aggregation, analysis, and visualization, supporting Docker container log storage.


8.) What is vertical scaling and horizontal scaling? Explain w/ a use case…
---------------------------------------------------------------------------
Vertical scaling: 
    - increases CPU/RAM capacity, exemplified by moving an EC2 instance from 
      Large to 2x.Large for enhanced features. 

Horizontal scaling:
    - manages incoming traffic, demonstrated by an Auto Scaling Group (ASG) handling 
      increased load while maintaining the same app features.


9.) Explain REPLICATION CONTROLLER in Kubernetes:
-------------------------------------------------
The Replication Controller in Kubernetes ensures the specified number of pods (replicas) 
from the YAML file are consistently present across the cluster.


10.) Do you use helm?
     versin currently using, and do you see benefits to using this particular 
     version?
-------------------------------------------------------------------------------
Yes, I’ve used v2 and v3.

Currently using v3.
Benefits include simplifying package management of Kubernetes apps in a cluster, enabling 
sharing apps as reusable chart versions. This facilitates easier deployment, upgrades, and rollbacks.

Additional Notes:
Helm uses YAML files for chart definitions and configurations.
Kubernetes uses YAML or JSON files for resource definitions (e.g., Pods, Services, Deployments).


11.) 
	a.) What is a Python Module?
		* Reusable files w/ Python code and data inside.
	
	b.) Explain what `if _ _name_ _ == “_ _main_ _”:  means?
        * That is a common construct that controls the execution of code. 
        * If I’m being run directly then execute said code. If I’m being imported - don’t execute me. 
        * The __name__ variable is a built-in Python variable that is automatically set by the Python interpreter.

	c.) Can you tell me what Boto3 is?
        * Boto3 is an (SDK) provided by Amazon for interacting with AWS services and resources.

	d.)Which python module would you use to make a simple API testing call.
	       Note: The code should just checkin an API endpoint is working or not.
        * I would use the requests module with “get” function check the status_code of the URL endpoint.


I.e,
import requests

url = 'https://example.com/api/endpoint'  # Replace with your API endpoint URL

response = requests.get(url)

if response.status_code == 200:
    data = response.json()  # Assuming the response is in JSON format
    print(data)
else:
    print(f'Error: {response.status_code}')

	E.) How would you setup a virtual environment in Python
		sh 'python3 -m venv myenv'
                sh 'source myenv/bin/activate'
                sh 'pip install -r requirements.txt'
                sh 'python3 my_test_script.py'
                sh ‘deactivate’


12.) What is remote-exec in Terraform and when will you use it?
        * It just executes a script on the remote resources after it’s created.
        * Let’s say we use TF code to spin up an EC2, then we can use remote-exec to execute a script. 

13.) Can we have Jenkins agent which is a docker container and run our test inside this docker container?
        * Yes, Standard way companies implement pipelines.
        * Usually the Jenkins servers are in an HA cluster (Master - Slave relationship)

14.) What are some way in which you have setup alerting?
We used Prometheus and through Alertmanager phone call and Email alerts would be triggered.  

        * Prometheus —> Will trigger alert through “Alertmanager”. 
        * Cloudwatch - SNS
        * Nagios - Alerting
        * Alert Notifications through: Email, mobile phone call, Slack Message.

15.) What are HELM Charts?
        * It’s a packaged application - pre-configured template w/ values. 

NOTE: Known Chart Repos
https://artifacthub.io/

Bitnami Charts (Bitnami):
* URL: https://bitnami.com/stacks/helm
* Description: Bitnami offers a collection of Helm charts for popular open-source software. These charts are often used in production environments and are known for their 

Helm Hub (Official):
* URL: https://hub.helm.sh/
* Description: The official Helm chart repository that provides a curated collection of charts maintained by the Helm project.

Extra:

HELM PRE-REQS
Add s3 Plugin to Helm
# helm plugin install https://github.com/hypnoglow/helm-s3.git

Add s3 to Helm Repo
# helm s3 init s3://<your-bucket-name>/<path-to-charts>
Note:
- S3 is Cloud Based Object Storage
- It uses HTTP for transport
- Offers a Restful API to interface with objects in the bucket. 

Note:
helm s3 init:
* helm s3 init is a command provided by the Helm S3 plugin.
* It is used to initialize an S3 bucket as a Helm chart repository. This means that you can use an S3 bucket to 
store and serve Helm charts.
* You specify the S3 bucket's path as the repository URL when using this command.
* Example:
# helm s3 init s3://my-helm-bucket/charts

helm repo add:
* helm repo add is used to add a remote Helm chart repository to your Helm client.
* It's typically used for public or private Helm chart repositories hosted on HTTP/HTTPS 
# helm repo add stable https://charts.helm.sh/stable

# helm repo update

PACKAGE HELM CHART
# helm package - -version <name>.v1.0.0 .     (Archiving Helm chart in current directory)

COPY CHART to S3 Bucket assuming AWS CLI is installed. 
# aws s3 cp /path/to/your/helm/chart.tar.gz s3://your-s3-bucket-name/path/to/destination/

DEPLOY CHART straight from Bucket
# helm install my-release s3://<your-bucket-name>/<path-to-charts>/<chart-name>


17.) Have you use Jenkins in multi node setup? If yes, explain how to add a new slave/follower to the master?
Directive: Practice setting up through Terraform.
* Yes I worked in a Jenkins Multi-node Setup. High Availability. 
* Log into Jenkins - Manage Jenkins - Manage Nodes and Clouds - Give Slave Info: (Node IP, PW, Username…”
Gets Register W/ Mater Node

a.) Can we add slaves dynamically?
* Sure, Add an Auto Scaling Group 

18.) Explain how you would block an IAM user from accessing a specific s3 bucket?
* Go into s3… Under Permissions / Bucket Policy…
* Mention the IAM User ARN and DENY Permission

19.) You encounter a docker image with size 2.7GB… Cause for Concern? 
* Depends on what the Application is being used for
* The Bigger it is - results in LONGER BUILD TIME, Download Errors.

A.) If yes, how would you tackle this?
* Use smaller Image (Alpine Image)      /// Light and doesn’t have unnecessary packages installed.
* Remove packages binaries and  remove packages that are NOT required. 

NOTE:
When IAM users or groups are created - they are assigned a unique identifier called  an ARN (Amazon Resource Name),

20.)  What does this IAM POLICY mean? Can you read it.

{ 
“version “ “ “ 2012-10-17”, 
“Statement”: [
     {
        “Sid”: “AllowS3ListRead”,  <——  Organized Unique identifier.  
        “Effect: “Allow:,		<——  Allow the following actions
        “Action”: [
              “s3:GetBucketLocation”,
              “s3:GetAccountPublicAccessBlock”,   <—— These are the actions we’re allowing.
              “s3:ListAccessPoints”


21.) What is PV and PVC in Kubernetes? What role do they play?
*  PV is a cluster wide resource and represents the physical storage (Disk or NAS/External Storage Device) for the cluster.
* PVC is a request for storage by a user or application running in a namespace (project).

22.) When using Terraform to create RDS, how do you save the DB username and password securely?
* Use a Hashicorp Vault to store creds, mention Vault in the terraform Provider Block
* DB’s are  used to retrieve info  from a data sources or query data (Internally/Externally).
* Write Data Block inside Main.tf requesting those secrets.   /// Data Block query/retrieve info from Data Resources. 

23.) What kinds of DB’s have you encountered, supported in the past?
- ETCD is a key-value store DB.
* It stores crucial configuration and state information of the Cluster. 
* Little of RDS/MySql
( Formulate more in-depth experience w/ MySql)

24.) Your security team has insisted that certain packages (Security) related ha to be  present on all the instances for defense project.

How would you come up with a solution for such a design and ensure these packages will be present on all EC2 Instances that shall be used? 
* Use Custom AMI’s as a solution…
* Packer enables you to create identical machine images for multiples platform from a single source template.
* JSON ——> Packer —-> OS Image.

25.) Why do we have 3 different kinds of IAM policy types?
- Managed Policy: Created and maintained by AWS

- Customer Managed Policy: Customer-managed policies are separate standalone policies that you can create, edit, and 
then attach to 
multiple IAM users, groups, or roles as needed. Will persist after IAM, U or G is deleted. 

- Inline Policy:  Inline policies can be created and are specific to an IAM user, group, or role. 
They cannot be reused or shared 
with other users, groups, or roles. When U,G or R is deleted so is the inline policy. 

26.) Which build tools are you aware of and what are the build tools used in the project?
Note:
> Build tools are critical for packing the application code and shipping it. 
> Can ship raw code files to servers/clients

Answer:
I have worked on multiple projects where the developers team used Java / NodeJS build tools

Java					NodeJS			Python
Maven, Gradle 			npm				PIP & Pybuilder 

NOTE: PLUGINS NEEDED FOR JENKINS
* Pipeline Plugin: 
* Github Plugin
* Git Plugin: 
* Maven Plugin: 
* Gradle Plugin:
* NodeJs Plugn
* Docker Puglin
* Surefire

1.) 
stage(‘Build’)
	sh 'mvn clean package'
- Maven compiles source code and any test source code. 
‘Sh ‘mvn clean package’
- clean: This removes any previously generated build artifacts.
- package: This rebuilds the project and packages it into its final distribution format.

2.) 
state(‘Test’)
JUNIT is a framework designed for writing and running test on small units of the code (Classes & Methods). 
Maven knows where to find the Junit test classes and main source code “src/test/java, src/main/java”
	sh ‘mvn test’
- Execute Unit Test - Test are written using the framework ‘JUnit’.
- Maven run all the TEST CLASSES
- Reports results to console
If EXIT CODE = 0   /// Application is deployed into 

Use this as part of STC Duties / Use AI to clear this up. Provide detailed scenarios…
Integration Tests: Integration tests come next in the testing hierarchy. These tests focus on ensuring that different parts 
of the application work correctly when integrated or connected. Integration tests verify interactions between components, modules, 
services, or even external systems. They test scenarios where data or control flow moves across various parts of the application.
* Example: In a web application, an integration test might involve testing how the frontend interacts with the backend API and how 
data is passed between them. This could include testing API endpoints, database connections, and the overall behavior of the application 
when these components are combined.


In order to run tests during the build process, it must use plugins like Surefire or Failsafe.


27.) What is ingress and egress?  Where are these terms mostly associated?
* Ingress: Incoming traffic
* Egress: Outgoing traffic
You see this term mostly associated w/ SECURITY GROUPS.

28.) What’s the difference between docker image vs docker layers?
SIDE NOTE: They’re completely different from one another. 
* A Docker Image is the complete package that defines the container
* The layers are the individual, reusable components that make up the image and store the changes.

NOTE:
If you do:
# docker history <image_name>    /// Show all the LAYERS that have been used.  

Example:
FROM rails:onbuild 						—> Use “onbuild” image as a starting point for the build. 
ENV RAILS_ENV dev						—> Setting the “RAILS_ENV” to “DEV”
ENTRYPOINT [“bundle”, “exec”, “logica”]		—> Tells Docker what command to run when Container is Started.

Each line in “Example” is considered a layer —> Creates an DOCKER IMAGE!


29.) What is a bastion host or gateway server? What role do they play?
NOTE: There are many names… Bastion Host, Gateway Server, Jump Box.
* A middle man between a public network and private. Once logged on - I can jump to in server in the private network. 
* From external network - manages access to a Private Network from External network. 
A.) How would stop the flow of traffic into the private network?
* Shut down the Bastion Host.

30.) Auto Scaling group for a project is having issues w/ getting/provisioning new nodes. It’s using complete spot instances. 
        What could be the issue?

* Spot Instances are spare compute capacity offered at a low cost by Amazon. On-Demand Instances.
1.) SPOT PRICE INCREASE /// Then Spot Instances can be taken away from you.
2.) EC2 quota limit. /// Could be a SOFT LIMIT set on the AWS Account. 


DOMAIN: Kubernetes
30.) A pod is trying to access a volume but it gives an access error. We want the pod to have access so what must we do?
Dose PV support the access mode we need. 
* Check the PV / PVC manifest are configured correctly - same “access mode” 
* Check pod configuration —> kubectl describe pod <pod_name>
	* Check for correct PVC Name, Filesystem Permissions, MountPath

31.) How is your K8’s setup done on AWS 
* Were using EKS for Production and Development Services (Why EKS? Flexibility, less maintenance.)
* The cluster builds are provisioned through Terraform.

32.) What are the load balancers in AWS? Briefly explain which ones you have used and their use cases?
* Classic Load Balancers
* Application Load balancer

Howto - CONFIGURE ALB
You config  the listeners on the ALB, create target groups for the listeners. ABL routes the traffic to the appropriate TGs. Assoc. TGs with the ASG,  register instances in the ASG w/ TG's assoc w/ ALB

In summary, the ALB acts as the entry point for incoming traffic and intelligently routes requests to the appropriate target groups associated with ASGs. It monitors the health of instances and ensures that traffic is evenly distributed while maintaining high availability and reliability for your applications hosted in the ASGs.

* Network Load Balancer
* Gateway Load Balancer

(ALBs)
Why do you used these????
We are a micro services shop so we use ALB’s a lot more
Because of their ability to handle microservices with advanced base routing.

                                      +--------------+
                                  |   Internet Gateway |
                                      +--------------+
                                                 |
                                    +------------------------+
                                    |   Gateway Load Balancer  |
                                    |        (GWLB)            |
                                    +------------------------+
                                                 |
                                    +------------------------+
                                    |   Firewall Appliance    |
                                    +------------------------+
							       I
							  VPC
                                                 		|
                                    +------------------------+
                                    |   Application Load      |
                                    |   Balancer (ALB)        |
                                    |                        |
                                    |   - Microservice A     |
                                    |   - Microservice B     |
                                    |   - ...                |
                                    +------------------------+
                                                 |
                                    +------------------------+
                                    |   Auto Scaling Group    |
                                    |   (Microservice A)     |
                                    |                        |
                                    |   - EC2 Instances      |
                                    |   - ...                |
                                    +------------------------+
                                                 |
                                    +------------------------+
                                    |   Auto Scaling Group    |
                                    |   (Microservice B)     |
                                    |                        |
                                    |   - EC2 Instances      |
                                    |   - ...                |
                                    +------------------------+

* Internet: This is where incoming traffic from the internet originates.
* Gateway Load Balancer (GWLB): The GWLB acts as the entry point to your Virtual Private Cloud (VPC) and routes traffic to the firewall appliance for inspection and filtering.
* Firewall Appliance: This is where your security policies and filtering rules are applied to incoming traffic. It ensures that only allowed traffic is forwarded to the Application Load Balancer.
* Application Load Balancer (ALB): The ALB is used to distribute traffic among your microservices based on advanced routing rules (e.g., path-based or host-based routing).
* Microservices (Microservice A, Microservice B, etc.): Behind the ALB, you have multiple microservices hosted in separate Auto Scaling Groups (ASGs). Each ASG contains EC2 instances that serve a specific microservice.
* Auto Scaling Groups: ASGs are responsible for automatically scaling the number of EC2 instances based on demand, ensuring high availability and scalability for your microservices.



Internet
   |
[GWLB]
   |
[Firewall]
   |
[VPC]
   | \
   |  \
   |   \
[ALB]  [Microservices]
   |     |
[Target Groups]

ALB Additional Interview questions.


* Question: Explain the difference between path-based routing and host-based routing in ALB. Answer: Path-based 
routing routes traffic based on the URL path Host-based routing routes traffic based on the hostname (subdomain) in the URL.

* Question: How can you implement cross-zone load balancing with ALB, and what are the benefits? Answer: Cross-zone load 
balancing is enabled by default. It evenly distributes traffic across instances in all availability zones. 

* Question: What is the difference between ALB and NLB in terms of layer and use cases? Answer: ALB operates at 
Layer 7 (application layer) - ideal for routing traffic based on content. 
Network Load Balancer (NLB) operates at Layer 4 (transport layer) and is designed for high-performance, low-latency 
applications like TCP/UDP load balancing.


33.) What is Sonarqube, have you used it?
It’s an automatic code review tool - detects bugs and vulnerabilities in the code. No I haven’t used it. 

Dev —> Checks the code into Github
	       Excuses a PR
	       Sonarq Checks the code for vulnerabilities and bugs
               Sends a report back - Pass or Fail
	       Peer review team steps in and reviews the code.
		+1 
	       Sonarq merges the code
		Jenkins job build the app

.34) How do you handle incidents in your team?
We monitor the environment with  Prometheus, Grafana. 

Application expose the —> Metrics —> Prometheus pulls Metrics —> Alert rules —> Triggers ONCALL (Email Slack & Phone)
* The App exposes the metrics
* We have Prometheus configured to pull the metric in.
* Based metric values, Alert rules will trigger an alert to the On-Call Engineer. /// (Phone Call / Slack)
* Engineer will come online, try to solve it or escalate to a higher level of support.

- Once Incident fix
- RCA Root Cause Analysis is performed
- Mitigation report is created with Solution


35.) Explain CMD and ENTRYPOINT. Are they the same or different. 
* They both specify what command should be executed on container startup. 
* They both can run the same commands, use the same shells, when used both together - you can pass values btw ea other.
* CMD is more of a default and you can change the behavior outside the container. You can’t do that  w/ ENTRYPOINT.
* Entry point is more of a cemented main process. Once you set it that’s it. 

36.) Multiple EDC2 instances in ASG is getting terminated and this is causing downtime on the application. EC2 pricing, quota all look good. How would you start debugging this issue. 

CHECK THE FOLLOWING:
* CPU
	# top 		/// Find the Offending process 
* DISK
	# df -h 
	# du -sh /var/log /tmp	/// Clear out some log space.
* MEMORY
	# free -mt		/// If SWAP = 0 That’s a problem too. App is trying to utilize all the mem. 

ASG —> EC2 —> Application —> unhealthy (ASG Terminates EC2) 

37.) What does terraform state lock really mean? Do we have a practical use of it?
When you execute 	///		# terraform apply
* Prevents concurrent access to the same Terraform state file - avoiding conflict and data corruption to the 
TF  infrastructure.

38.) Currently you have a Jenkins job, but we don’t have any kind of notifications on job success or failures. 
We use soak for our internal communications.
How would you enable a notification service in this case?

* Install the Email and Notification Plugin for SLACK.
* Configure with Authentication Token and Slack Webhook.

39.) In a Ansible playbook, We have a section which could fail on certain nodes. But we don’t want the playbook 
to stop or exit b/c of this.
Is it possible to ignore this if this part of the playbook fails. 

* Yes, by using the “ignore_errors sub  argument”
- name: Do not count this as a failure
	ansible.builtin.command: /bin/false
		ignore_errors:  yes

40.) If you were to make a change for a commit done 30 days ago with respect to a machine type used?
How would you go about doing this change.
Would you directly change it?

* No, I wouldn’t directly change it w/o knowing to the history behind it. 
- Got to repo (Github UI) and check GIT BLAME for history —> and then possibly make those changes. 
a.) # git clone on the Repo
b.) Create a new branch
c.) Apply the changes

41.) Create a script that will push certain logs to S3 automatically.
Explain only high level design which is enough. Share what all steps you would do to achieve this above script. 

Note: You can used Python “Time” module or put Python script in Linux Cron Job.

import boto3
import os
import glob
import time

# Initialize the S3 client
s3 = boto3.client('s3')

# Specify the local directory where your log files are located
log_directory = '/path/to/log/files/'

# Specify the S3 bucket name and prefix (folder) where you want to store the logs
bucket_name = 'your-s3-bucket-name'
s3_prefix = 'logs/'

# Specify the file extensions of the logs you want to upload
log_extensions = ['.log', '.txt']

# Calculate the time until the next scheduled execution (in seconds)
# For example, to run the script at 2:00 PM, set the desired_hour to 14 (24-hour format)
desired_hour = 14  # Adjust this to your desired hour
current_hour = time.localtime().tm_hour
seconds_until_execution = (desired_hour - current_hour) * 3600  # 3600 seconds in an hour

# Sleep until the desired execution time
if seconds_until_execution > 0:
    print(f"Sleeping for {seconds_until_execution} seconds until {desired_hour}:00 PM...")
    time.sleep(seconds_until_execution)

# Get a list of log files matching the specified extensions in the local directory
log_files = [f for f in glob.glob(os.path.join(log_directory, '*')) if os.path.splitext(f)[1] in log_extensions]

# Iterate through the log files and upload them to S3
for log_file in log_files:
    # Extract the file name without the path
    log_filename = os.path.basename(log_file)
    
    # Construct the S3 object key (path in the bucket)
    s3_object_key = s3_prefix + log_filename
    
    # Upload the log file to S3
    s3.upload_file(log_file, bucket_name, s3_object_key)

print("Log files uploaded to S3 successfully.")


42.) Explain in simple terms what is a package in Python?
* Python package is  a collection of Python modules in a directory hierarchy starting with a special file __init__.py
- Modules have functions
* A Python Module = It a file with Python code, variables and data. 
* “OS” is a package, ea module has a particular usecase and multiple functions available.

IMPORTANT QUESTION WILL BE ASKE FIRST….
43.) Tell me about yourself and what your day to day task being a DevOps engineer?

43/A.) What Is DevOps
It’s a methodology, a culture. A collaboration between Developers and IT operation to stream line code through 
Automated testing and deployment. Bottom line - Accelerate Software Delivery. 

My day starts at 9am 
- Check my slack messages
- Update my Jira tickets 
- We have a daily standup (Discuss what I’ve done, what I’m working on today
- After, I’ll got work on my Jira tickets and latest request.
* Code Reviews
* AWS request 
* Making Operations changes
* Work with devs on: Design discussion and request to setup integrations test in the Kubernetes environment. 
* Maven projects

Domain: K8s
44.) We want to launch certain pods only on specific nodes. These nodes should explicitly use only the pods. 
Is it possible to control the deployment of pods in such a given scenario?

* You use “NODE TAINTS”
# kubectl taint nodes <node-name> <taint-key>=<taint-value>:NoSchedule
Pod w/ a matching toleration will deploy on the tainted node.
Pod w/o will be repelled from tainted node.


apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
  tolerations:
  - key: <taint-key>
    operator: Equal
    value: <taint-value>
    effect: NoSchedule

45.) Let’s say you have 2 servers with different ports and username then how can ansible handle different 
machines for applying and running the same runbook?

* Create a separate inventory group for each server w/ specific connection details. Then specify the target 
those target groups when running the playbook.

Example:

Suppose you have two servers:

Server 1:

IP Address: 192.168.1.100
SSH Port: 2222
Username: user1

Server 2:

IP Address: 192.168.1.101
SSH Port: 22
Username: user2


Create an Ansible inventory file (e.g., inventory.ini) with the following content:
————————————————————————
[web_servers]
192.168.1.100:2222 ansible_ssh_user=user1
192.168.1.101 ansible_ssh_user=user2
———————————————————————

Now, you can run your Ansible playbook against these servers. For example, if your playbook is named 
my_playbook.yml, you can run it like this:

# ansible-playbook -i inventory.ini web_servers my_playbook.yml



46.) What is the difference between a Web Server and Application Server?

* Web servers serve web content, while application server on the backend runs web app logic.

47.) How can we handle secrets in ansible? Let’s say we have a playbook which needs to login to premises 
server with login name and password

* # echo “data to be  encrypted’> encrypt_file.txt
* # ansible-vault encrypt encrypt_file.txt
* Or integrate it with Hashi Corp Vault

48.) What is a Blue / Green deployment?
It’s a software release strategy where you have 2 identical environments. Blue representing the current 
running  prod env and green contains the updated release. You switch over the traffic to Green for testing 
and can easily be rolled back. 


Blue —> Running  v1.0
[ Network Switch is pointing to BLUE ]   (Routing Control by ROUTE53 or F5)

- New deployment =  1.01
- Deploy on “Green”
Green —>  now Running v1.01

- Route Traffic from Blue to Green
[ Network Switch is pointing to GREEN ]	(Routing Control by ROUTE53 or F5)

- Now we MONITOR the GREEN Environment
- Any Issue: Change network config to roll back to BLUE .


49.) What does a Mult-Branch Pipeline project intel?

You need different branches in your git repo. W/ separate Jenkins Files
- Master - JenkinsFile
- Staging - JenkinsFIle
- Dev - JenkinsFile

.50) What is a Market Place AMI
It’s where you can find external software that can be used w/o signing a specific contract. 
It’s basically how 3rd party companies earn money
* You spin up a EC2 instance - Cost paid to AWS
* You use say RedHat AMI with that EC2 - Cost is paid to Redhat.

51.) A dev team wishes to go with a NoSQL DB, but what is this NoSQL DB and do we have something in AWS 
that can be used as a NoSQL DB?

Store large volumes of un-structured data, no fix schemas. 
* AWS DynamoDB (key/value Document Store), DocumentDB

52.) We have an on-premises linux server which needs some monitoring enable. Being a DevOps engineer only 
you have access to this server. How would you setup basic monitoring script on it?

Write a Python script w/…
* Import psutil                 // This python module can monitor any kind of server resources. 
* Exist in Pipit package
* # pip install psutil
* Cpu, load, memory Disk usage, Disk I/O

53.) Why is logging important? What is centalised logging and what tools help to achieve centralized logging?

* Logging can help you debug and fix issue pertaining to your application. 
* Centralize logging you can dump to: Splunk, S3,  CloudWatch, Elastic Search, DataDog(Log Aggregator)

54.) What is a side car container?
* Supports the main application container’s operations through monitoring and logging.
* It’s a small, supplementary container that runs alongside the main application container. 
NOTE:
Multiple container running in a POD can be destroyed at any time  so their logging is directed
To the SIDE CAR CONTAINER. 

55.) Terraform apply is failing. The process is in a hung state. Now if you cancel this it might result 
in corrupt terraform state.
How would you debug this issue? 

* I’d save the remote statefile first
- Connect to a new Terminal 
- terraform state pull     ///    This pulls remote statefile to your local desktop filesystem like a 
backup an avoiding corruption. 
- Kill the process “terraform apply” and run it again. 
- If it doesn’t work then I’ll just replace the current statefile w/ my backup. 

DOMAIN: Github
56.) What is branch protection in GitHub?

Discussion:
This is the proper way to merge code…
Working branch —> You make changes —> Open a PR —> Get a review —> Merged Code to Master/Main Branch
- If there are no protections in place then a dev can just BYPASS (PR & Review) and merge the code. This is incorrect!

* You must enable “Branch Protection Rules”
- Goto Master Repo, Click Settings, Click Branches, Click Add branch protection rules, Check “Require Pull 
Request B4 Merging” (Requiring x-number of approvals.)

(DOMAIN: Kubernetes)
57.) How does the K8’s scheduler quickly assign worker nodes for pods?
Explain the internal workings of K8’s scheduler.

Control Plane Consist of:

* Kubernetes API Server (kube-apiserver): This is the front-end for the control plane. It exposes the 
Kubernetes API, which is used by other components and users to interact with the cluster.

* etcd: This is a distributed key-value store that stores the cluster's configuration data and state. 
It is the single source of truth for the cluster.

* Kubernetes Controller Manager: This component runs controller processes that regulate the state of the system. 
Examples include the Replication Controller and Node Controller.

* Kubernetes Scheduler: The scheduler is responsible for placing pods onto nodes based on resource requirements, 
node constraints, and other policies.


SHORT:
The Kubelet reports node information to the K8’s API server, which in turn stores the data in “etcd” database
and retrieves this data whenever the scheduler request it. 

LONG:
The kubelet on each worker node monitors the node's health, including resource usage (such as disk space, memory, 
and OS health), and reports this information to the Kubernetes API server. The API server stores this data and uses 
it for various cluster management tasks, including scheduling decisions and node health monitoring.

- Two Step Process:
1.) Filtering
The scheduler filters out any unsuitable nodes based on POD resource requirements. 

2.) Scoring
The scheduler picks the node with the best score which is based on resources, load and what have you.


58.) Please explain how to create your dashboard for monitoring services in your current project.
- Prometheus:

- Open a web browser and navigate to Grafana Web UI
- 3 Clicks: (Acronym C-D-PANEL)
- ‘Create’ menu
- ’Dashboard’
- ‘Add New Panel’, EDIT
			* Choose Data Source: Prometheus
			* Write your PromQL Query to fetch the metrics
			* http_requests_total{job="my-app", port="80"}
NOTE:
This query retrieves the total HTTP requests metric for a job named "my-app" running on port 80.

- Visualization Section: Choose your Visual Representation. 
- SAVE the PANEL and DASHBOARD
- Access and share the DASHBOARD

58B.) HOW WOULD YOU DO IT FOR ‘CLOUD WATCH’?

DOMAIN: Git + Terraform
59.) Explain how Terraform plan + apply happens when using CI/CD to provision a AWS/GCP infrastructure in your 
current project?
- We’ll have Jenkins Installed a Dockerized, Jenkins agent. 
- TF configs and Jenkins files will live the Said Github repo
- Upon code changes, PR executed and then Code Review. If successful Merge code into Main Branch
- CI/CD Job is triggers following the Init, Plan, Apply step defined in Jenkins file. 
- Infrastructure build begins. 
Doing it this way… you achieve automation, consistency, and traceability in the CI/CD pipeline.

Note: There will be 100’s of TF Repos


Let's assume I already have the Jenkins Server setup. In fact it's a Jenksins multinode cluster setup with 1 master 
and two slaves and now we add a Jenkins/agent container with Terraform installed on it. Well assign AWS Role which 
gives permissions to the container to have access to AWS Cloude and it's resources while running the Terraform code. 
Questions now... Does the Jenskins master have to be configured with the Jenkins/agent container as an agent? Is 
show show me the steps.


This Assumes:

  1. Provision an EC2 instance with Docker installed.
2. Create a Dockerfile and build your Jenkins agent container on the EC2 instance.
3. Push the agent container image to a container registry (optional) or use it directly on your Jenkins master server.
4. Configure Jenkins to use the Jenkins agent container on your Jenkins master server as needed.
5. Create AWS ROLE w/ all req perms are allocated for the Container to access AWS and perform the TF Build.
6. Setup TF CODE and Jenkins File in Github repo
7. Setup CI/CD Job and Triggers.

Now, your Jenkins master is configured with the Jenkins agent container as a new agent. Jenkins will use this 
agent for job execution based on labels and availability settings. The Jenkins master will distribute jobs to 
this agent just like it does for other agents in the Jenkins cluster.
Ensure that the agent's IAM role is properly configured to allow the agent to interact with AWS resources, 
as you mentioned, and that the agent's label matches the labels used in your Jenkins jobs.

SCENARIO PIPELINE

- DevOps Eng. Checks out a Feature branch of TF and Commit some changes
- Executes a PR request
- Code Review ensues 
- Branch is merged into the Master Branch - CI JOB Kicks off
- TF Code and the Jenkins file get checked out by Jenkins. 


HOW OUR JENKINS FILE SHOULD LOOK 

pipeline {
    agent {
        label 'my-agent-label' // Specify the label of your Jenkins agent
    }
    environment {
        AWS_DEFAULT_REGION = 'us-west-2'
    }

    stages {
       stage ( SCM Checkout Code ) {
           steps {
           }
              
        stage('Start Jenkins Agent Container') {
            agent {
                docker {
                    image 'my-jenkins-agent:latest' // Replace with the name of your Jenkins agent Docker image
                    args '-e JENKINS_URL=http://jenkins-server:8080 -e JENKINS_AGENT_NAME=my-agent-name'
                }
            }
            steps {
                script {
                    // Assume the AWS IAM role here, if not already configured in your Docker agent container
                    sh 'aws sts assume-role --role-arn arn:aws:iam::123456789012:role/JenkinsRole --role-session-name JenkinsAssumeRole --duration-seconds 900 > role-credentials.json'
                    withCredentials([file(credentialsBinding: 'role-credentials.json', variable: 'ROLE_CREDS')]) {
                        // No need to export AWS access keys or session tokens here
                    }
                }
            }
        }

        stage('Terraform Init') {
            steps {
                script {
                    // Initialize Terraform
                    sh 'terraform init'
                }
            }
        }

        stage('Terraform Plan') {
            steps {
                script {
                    // Generate a Terraform execution plan
                    sh 'terraform plan'
                }
            }
        }

        stage('Terraform Apply') {
            steps {
                script {
                    // Apply the Terraform changes (use --auto-approve to avoid interactive confirmation)
                    sh 'terraform apply --auto-approve'
                }
            }
        }
    }
    post {
        success {
            // Additional steps to run on successful deployment
        }
        failure {
            // Additional steps to run on deployment failure
        }
    }
}


To summarize, your Jenkins agent container, when started and configured with the Jenkinsfile, will assume the specified 
AWS IAM role for AWS interactions, including Terraform deployments, within your CI/CD pipeline.

DOMAIN: K8’s
60.) Can we control Namespace Utilization Per Name space in K8’s?
- It’s a virtual cluster or a logical partition or a means of isolating you IT workloads. 
- Yes you can control resource utilization through K8’s resource quota yaml file.

apiVersion: v1
Kind: ResourceQuota
metdata:
   name: my_name_space
spec:
  hard:
     requests.cpu:”1”
     request.memory: 1 Gi
     limits.cpu: “2”
     limits:memory: 2Gi

# kubectl get resourcequota men-cpu-demo - - 
   namepspace=my_name_space - -output=yaml


DOMAIN: Github
61.) We have a commit that was pushed to the main branch, we don’t want it now, we have to quickly remove this 
commit to avoid it getting deployed in Production. 
       
How would you do this???

- We need the “Commit ID#”
# git log - -online    (Commit ID#)
# git revert <CommitID#>
# git push origin

* git reset: This command moves the HEAD and branch pointers to a specified commit, effectively "rewinding" 
the branch history. It's often used to undo local commits.

* git revert: This command creates a new commit that undoes the changes made by a previous commit. It is 
a safe way to undo changes, especially when working in a shared repository.

* HEAD: HEAD is a symbolic reference in Git that points to the most recent commit in the current branch. 
It represents the snapshot of your last commit.

* Cherry Picking: The process of selecting a specific commit from one branch and applying it to another branch. 
It allows you to choose and merge individual commits rather than merging entire branches.


Note: 
What is “git reset”?
What is “Cherry Pick”?
Or you can create a New PR and change it. 


62.) What is the best use case of Dockerhub web hook integration?

Configure your Container registry to send Webhook events. Configure your CI/CD pipeline to listen for those webhook events. 
So when a docker image is pushed to the registry, your CI job kicks off, spins up the 
container in a staging environment where Unit and Integrations test will be performed. 


63.) What is CRON and ow does it work in Linux?
- Is a Job Scheduler
- Crond is the daemon runs in background, reads crontab files containing jobs. Runs the jobs
       the scheduled jobs. 

Access Crontab File
# crontab -e

Log File Location
—————————
/var/log/syslog
/var/log/cron.log

63b.) How would you know a cron job has failed. 
Checking the logs.



STUDY:

HTTP METHODS:
https://www.youtube.com/watch?v=PO7D20HsFsY	(PRETTY GOOD)

APACHE on UBUNTU:
How to Install SSL Certificate on Linux Apache Web Server
https://www.youtube.com/watch?v=m06TttS2ZAM

* Global Apache configuration file  ...  "/etc/httpd/conf/httpd.conf"
* DocumentRoot ...  "/var/www/html/ "
   - Directory where data will be served to visitors.
   - You will be able to access that data via a web browser using your 
      server IP address.

Creating virtual hosts for multiple domains:
https://support.rackspace.com/how-to/serve-multiple-domains-by-using-virtual-hosts/
RUN QUICK TEST ON MULTIPLE VHOST WITHOUT ADDING 
DNS records(Mapping Domain to IP)

$ curl -H "Host: mysite.com" http://10.20.30.40
    - This is especially useful for quick troubleshooting and you don't have 
       to remove   anything after you are done.
---------------------------------------------------------


- Restart Apache after you have made changes 
  and doing diagnosis ...

$ httpd -k restart            
$ apache2 -k restart

$ httpd -S                    / Print out Vhost config info



LINUX BOOT PROCESS:
BIOS  
a.) Searches for Boot Disk
b.) Executes MBR
 	- located First Sector of Boot Disk
	- Commponents 
            	* Boot Loader Info
		* Partition Table
		* Validation Check
MBR 
a.) Executes Grub  --> /boot/grub/grub.conf
b.) Splahs Screen
c.) Choose between multiple kernels to load

GRUB (Grand Unified Bootloader)
a.) Executes Kernel
b.) Loads intitrd
        - Temp filesystem loaded into memory
       - Performs Hardware Detection
       - Loads Kernel modules and actual root filesystem mounted.
(initrd, kernel stored in /boot
KERNEL
a.) Executes /sbin/init  or systemd

INIT/SYSTEMD
/etc/inittabl -- DECIDES LINUX RUNLEVEL
RUNLEVEL DECIDES WHICH PROGRAMS WILL RUN

RUNLEVEL
System Executes Programs pertaining to RUNLEVEL


WEB API /


STUDY:

HTTP METHODS:
https://www.youtube.com/watch?v=PO7D20HsFsY	(PRETTY GOOD)


Apache on Ubuntu
----------------

How to Install SSL Certificate on Linux Apache Web Server
https://www.youtube.com/watch?v=m06TttS2ZAM

yum update
iptables -F
yum install httpd
service start httpd
service enable httpd


Global Apache configuration file in CentOS 7 is /etc/httpd/conf/httpd.conf

DocumentRoot which is the directory out of which the data will be served to 
the visitor. Usually, in most distros the default document root for Apache is 
set to /var/www/html/ so if you put data in /var/www/html/ you will be able 
to access that same data via a web browser using your server IP address.

Create Virtualhost:

1.) 
vi /etc/httpd/conf.d/your_domain.conf


------------------------------------------------------
RUN QUICK TEST ON MULTIPLE VHOST WITHOUT ADDING 
DNS records(Mapping Domain to IP)

$ curl -H "Host: mysite.com" http://10.20.30.40

This is especially useful for quick troubleshooting.
And you don't have to remove anything after you are done.
---------------------------------------------------------



Troubleshooting:
resource: https://support.rackspace.com/how-to/serve-multiple-domains-by-using-virtual-hosts/

- Restart Apache after you have made changes 
  and doing diagnosis ...

$ httpd -k restart            
$ apache2 -k restart

$ httpd -S                    / Print out Vhost config info
$ apache2 -S


JSON:

Q: What does JSON stand for?

A: JavaScript Object Notation.
Q: Describe the basic structure of JSON.

A: It consists of key-value pairs, where keys are strings and values can be strings, numbers, objects, arrays, booleans, or null.
Q: How do you represent an array in JSON?

A: By enclosing values in square brackets, like [value1, value2, ...].
Q: Can JSON represent complex data structures?

A: Yes, through nested objects and arrays.
Q: What is the file extension commonly used for JSON files?

A: .json.
YAML:

Q: What does YAML stand for?

A: YAML Ain't Markup Language (or sometimes, YAML Ain't a Markup Language).
Q: Describe the basic structure of YAML.

A: It uses indentation to represent the hierarchy and relies on key-value pairs. Lists are represented using a dash followed by a space.
Q: How does YAML handle comments?

A: Comments start with a hash (#) symbol.
Q: What is the file extension commonly used for YAML files?

A: .yaml or .yml.
Q: Can YAML represent associative arrays?

A: Yes, YAML uses mappings to represent associative arrays or dictionaries.



MORE Q/A CI/CD PIPELINE -- DEVOPS

CI/CD Concepts:

Can you explain the concepts of Continuous Integration and Continuous Deployment?
How do CI and CD contribute to the software development lifecycle?
What are the benefits of implementing CI/CD in a development process?
CI/CD Tools:

Which CI/CD tools have you used in your previous roles?
Can you describe a scenario where you implemented or improved a CI/CD pipeline?
What considerations do you take into account when selecting CI/CD tools for a project?
GitHub:

How do you use Git in your day-to-day work?
Can you explain the difference between Git and GitHub?
Have you worked with GitHub Actions? If so, how?
Build Automation:

What build tools have you used in conjunction with CI/CD pipelines?
How do you handle versioning and dependency management in your build process?
Deployment Strategies:

Explain different deployment strategies, such as blue-green deployment or canary deployment.
How do you ensure zero-downtime deployments?
Infrastructure as Code (IaC):

Have you used tools like Terraform or Ansible for infrastructure provisioning?
How does Infrastructure as Code contribute to a DevOps environment?
Monitoring and Logging:

How do you monitor the health of a system in a CI/CD environment?
What tools or practices do you follow for logging and monitoring in a production environment?
Collaboration and Communication:

How do you facilitate collaboration between development and operations teams?
Can you share an example of resolving conflicts between different teams within a DevOps context?
Security in CI/CD:

How do you address security concerns in a CI/CD pipeline?
What measures do you take to ensure the security of code and deployment processes?
Continuous Learning:

How do you stay updated on the latest trends and best practices in DevOps?
Can you share an example of a challenging problem you faced and how you approached solving it?
These questions cover a broad range of topics within the DevOps domain, and they aim to assess 
both technical knowledge and practical experience in implementing DevOps best practices. 
Tailor the questions based on the specific tools and technologies mentioned in the job description.

User
Now provide short precise answer to all those questions
ChatGPT
CI/CD Concepts:

Answer: CI/CD is a software development practice that involves automatically integrating 
code changes (CI) and deploying them to production (CD), ensuring frequent and reliable releases.
CI/CD Tools:

Answer: I have experience with Jenkins and GitLab CI/CD. In a previous role, I enhanced 
a Jenkins pipeline to automate builds, tests, and deployments.
GitHub:

Answer: I use Git for version control. GitHub is a web-based platform for hosting 
repositories and collaborative development. Yes, I've used GitHub Actions for CI/CD.
Build Automation:

Answer: I've utilized tools like Maven and Gradle for build automation. Dependency 
management is crucial, and I ensure dependencies are defined and versioned appropriately.
Deployment Strategies:

Answer: Blue-green and canary deployments are strategies for minimizing downtime 
during releases. Blue-green switches between two identical environments, while canary 
gradually rolls out changes to a subset of users.
IaC (Infrastructure as Code):

Answer: I've employed Terraform for infrastructure provisioning. IaC enables defining 
and managing infrastructure in a declarative manner, enhancing consistency and scalability.
Monitoring and Logging:

Answer: I use tools like Prometheus for monitoring and ELK Stack for logging. 
Monitoring involves tracking system health, and logging captures and analyzes application logs.
Collaboration and Communication:

Answer: Communication is facilitated through tools like Slack and regular cross-functional 
team meetings. I've resolved conflicts by fostering a culture of open communication and shared goals.
Security in CI/CD:

Answer: Security scans are integrated into the CI/CD pipeline using tools like 
SonarQube. Secure coding practices, regular vulnerability scanning, and code reviews 
contribute to a secure pipeline. Continuous Learning:

Answer: I stay updated through industry blogs, webinars, and conferences like DevOpsDays. 
When faced with challenges, I collaborate with the team and leverage online resources for solutions.



AWS ALB / ECS / DOCKER / MICROSERVICES:

Application Load Balancer (ALB). In this example, we'll use AWS ECS (Elastic Container Service) 
for container orchestration.

Microservices Application Components:
Microservice 1 - User Service:

Endpoint: /users
Functionality: Manages user information.
Microservice 2 - Order Service:

Endpoint: /orders
Functionality: Handles order-related operations.
AWS Infrastructure Components:
Amazon ECS Cluster:

Set up an ECS cluster to manage containerized microservices.
Docker Containers:

Package each microservice as a Docker container.
ECS Service and Task Definitions:

Create ECS task definitions and services for each microservice.
Application Load Balancer (ALB):

Create an ALB to distribute traffic to microservices based on path-based routing.
ALB Configuration:
Listener Rules:

Configure path-based routing rules on the ALB to direct traffic to the corresponding 
microservice based on the path in the URL. 

Copy code
Rule 1: Path is /users/*  -> Target Group: UserMicroserviceTargetGroup
Rule 2: Path is /orders/* -> Target Group: OrderMicroserviceTargetGroup
Target Groups:

Create two target groups, one for each microservice.
UserMicroserviceTargetGroup
OrderMicroserviceTargetGroup
Container Port Mapping:

Configure ECS task definitions to expose specific ports for each microservice.
User Service: Expose port 5000
Order Service: Expose port 6000
Service Discovery:

Enable service discovery on ECS so that the ALB can dynamically discover and register 
containers as they scale.

Deployment Steps:
Containerize Microservices:

Build Docker images for each microservice.
Deploy ECS Task Definitions and Services:

Deploy ECS task definitions and services for the User and Order microservices.
Configure ALB:

Set up the ALB with listener rules, target groups, and health checks.
Scale Services:

As the microservices scale based on demand, ECS will dynamically register instances with the ALB.
How ALB Operates:
When a request is sent to the ALB, it examines the path in the URL.
Based on the configured listener rules, the ALB routes the request to the appropriate target group.
The target group forwards the request to the ECS service managing the specific microservice.
ECS dynamically scales the microservices based on demand, and the ALB adjusts its target group accordingly.
Example Requests:
User Service: http://alb-url/users/getAll
Order Service: http://alb-url/orders/create
Benefits:
Scalability: ECS scales microservices based on demand, and ALB dynamically adjusts to changes in 
the number of instances.
Flexibility: Path-based routing allows for a flexible and easy-to-manage configuration.
High Availability: ALB supports high availability with multiple availability zones and automatic failover.
This example illustrates a basic setup for a microservices application with an ALB on AWS using ECS. 
The specific details may vary based on your application requirements and architecture.

User
Target groups are defined where?
ChatGPT
Target groups in AWS are defined and configured within the context of an Application Load 
Balancer (ALB). When you create an ALB, you also create one or more target groups associated 
with it. Target groups are used to route requests to registered targets, which can be instances, 
containers, IP addresses, or Lambda functions.

Here's a step-by-step guide on where and how target groups are defined:

Create an Application Load Balancer (ALB):

Go to the AWS Management Console.
Navigate to the EC2 service.
In the left navigation pane, choose "Load Balancers."
Click on the "Create Load Balancer" button.
Select "Application Load Balancer" as the type.
Configure Load Balancer Settings:

Provide a name for the ALB.
Configure listeners (e.g., HTTP on port 80).
Configure security settings, subnets, and other relevant options.

Configure Routing:

In the "Configure Routing" section, set up listener rules.
Listener rules define conditions for routing requests to target groups based on factors 
like the path in the URL.

Define Target Groups:

In the "Add a listener rule" or "Add another listener" section, you'll have the option to 
create or select an existing target group.
Click on "Create a target group" to define a new target group.
Configure Target Group Settings:

Give the target group a name.
Set the protocol and port for communication between the ALB and the targets.
Specify health check settings to determine the health of the registered targets.
Register Targets:

After creating the target group, you can register targets (e.g., EC2 instances, containers) with it.
Targets are instances or services that will receive and respond to requests.
Review and Create:

Review your ALB configuration, including target groups and listener rules.
Click on "Create" to create the ALB with the defined settings.
The target groups you create are specific to the ALB they are associated with. You can have 
multiple target groups associated with a single ALB, and each target group can have its own 
set of targets and health check settings.

After the ALB is created, it dynamically routes incoming requests based on the defined listener 
rules to the appropriate target group, and the target group forwards the requests to registered targets.

It's important to note that the actual steps and user interface details might vary 
slightly based on updates to the AWS Management Console. Always refer to the AWS 
documentation for the most accurate and up-to-date information.
