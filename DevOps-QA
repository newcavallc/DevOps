


1.) EC2 Instance Running Out of Disk Space:
-------------------------------------------
    Check /root - os:
           - Investigate and clear space in /root.
    Check the logs:
           - Examine and free up space in /var/log and /tmp.
    Check /application - for our application:
           - Inquire about the application's purpose and log location.
           - Explore clearing space in the application directory.
           - If necessary, take an EBS Snapshot and increase the associated volume size.


2.) Prometheus obtains metrics in two ways:
-------------------------------------------
    Pull Approach: Exposes metric endpoints (e.g., Java App/Node.js) for Prometheus to scrape data.

    Push Approach: The App sends metrics via a gateway back to Prometheus for storage.

    Q: Why choose PUSH?
    A: For apps that may not run continuously, PUSH ensures timely metric updates.

Prometheus is a toolkit for monitoring and alerting, ensuring system and application reliability. 
It collects metrics to inform decision-making, supporting dashboards and alerting. It monitors Docker 
containers, capturing CPU usage, memory, network traffic, and more, with built-in support for exporters 
like Node Exporter and Docker.


3.)What is Kubernetes kOpt (Kubernetes Operations)?
---------------------------------------------------
Kubernetes kOps is not limited to testing; it is a versatile tool used for production deployment and 
management of Kubernetes clusters.


4.) What is an instance fleet in AWS?
-------------------------------------
An AWS instance fleet is a diverse set of EC2 instances designed for specific use cases:
    * pricing models
    * Amazon EMR (Cloud-based big data platform) 
    * Application that require tailoring.


5.) Are you supporting any applications? If yes what application sever?
-----------------------------------------------------------------------
No, I haven't worked with application servers for:

Java: (Tomcat, JBoss, Weblogic)
NodeJS: (IIS)
Python: (Django)


6.) How to check logs on a Docker container - the last 200 lines?
-----------------------------------------------------------------
To check logs and filter only the last 200 lines, use the following command:
# docker container logs <container_name> --tail 200

This command will display the last 200 lines of the container logs.


7.)  What will happen to container logs if you restart the container? Will it be lost?
--------------------------------------------------------------------------------------
Restarting a container won't result in log loss; data is only lost upon container deletion 
due to their statelessness.

Consider using external persistent storage like SPLUNK for app logs. SPLUNK is effective for 
log aggregation, analysis, and visualization, supporting Docker container log storage.


8.) What is vertical scaling and horizontal scaling? Explain w/ a use case…
---------------------------------------------------------------------------
Vertical scaling: 
    - increases CPU/RAM capacity, exemplified by moving an EC2 instance from 
      Large to 2x.Large for enhanced features. 

Horizontal scaling:
    - manages incoming traffic, demonstrated by an Auto Scaling Group (ASG) handling 
      increased load while maintaining the same app features.


9.) Explain REPLICATION CONTROLLER in Kubernetes:
-------------------------------------------------
The Replication Controller in Kubernetes ensures the specified number of pods (replicas) 
from the YAML file are consistently present across the cluster.


10.) Do you use helm?
     versin currently using, and do you see benefits to using this particular 
     version?
-------------------------------------------------------------------------------
Yes, I’ve used v2 and v3.

Currently using v3.
Benefits include simplifying package management of Kubernetes apps in a cluster, enabling 
sharing apps as reusable chart versions. This facilitates easier deployment, upgrades, and rollbacks.

Additional Notes:
Helm uses YAML files for chart definitions and configurations.
Kubernetes uses YAML or JSON files for resource definitions (e.g., Pods, Services, Deployments).


11.) 
	a.) What is a Python Module?
		* Reusable files w/ Python code and data inside.
	
	b.) Explain what `if _ _name_ _ == “_ _main_ _”:  means?
        * That is a common construct that controls the execution of code. 
        * If I’m being run directly then execute said code. If I’m being imported - don’t execute me. 
        * The __name__ variable is a built-in Python variable that is automatically set by the Python interpreter.

	c.) Can you tell me what Boto3 is?
        * Boto3 is an (SDK) provided by Amazon for interacting with AWS services and resources.

	d.)Which python module would you use to make a simple API testing call.
	       Note: The code should just checkin an API endpoint is working or not.
        * I would use the requests module with “get” function check the status_code of the URL endpoint.


I.e,
import requests

url = 'https://example.com/api/endpoint'  # Replace with your API endpoint URL

response = requests.get(url)

if response.status_code == 200:
    data = response.json()  # Assuming the response is in JSON format
    print(data)
else:
    print(f'Error: {response.status_code}')

	E.) How would you setup a virtual environment in Python
		sh 'python3 -m venv myenv'
                sh 'source myenv/bin/activate'
                sh 'pip install -r requirements.txt'
                sh 'python3 my_test_script.py'
                sh ‘deactivate’


12.) What is remote-exec in Terraform and when will you use it?
        * It just executes a script on the remote resources after it’s created.
        * Let’s say we use TF code to spin up an EC2, then we can use remote-exec to execute a script. 

13.) Can we have Jenkins agent which is a docker container and run our test inside this docker container?
        * Yes, Standard way companies implement pipelines.
        * Usually the Jenkins servers are in an HA cluster (Master - Slave relationship)

14.) What are some way in which you have setup alerting?
We used Prometheus and through Alertmanager phone call and Email alerts would be triggered.  

        * Prometheus —> Will trigger alert through “Alertmanager”. 
        * Cloudwatch - SNS
        * Nagios - Alerting
        * Alert Notifications through: Email, mobile phone call, Slack Message.

15.) What are HELM Charts?
        * It’s a packaged application - pre-configured template w/ values. 

NOTE: Known Chart Repos
https://artifacthub.io/

Bitnami Charts (Bitnami):
* URL: https://bitnami.com/stacks/helm
* Description: Bitnami offers a collection of Helm charts for popular open-source software. These charts are often used in production environments and are known for their 

Helm Hub (Official):
* URL: https://hub.helm.sh/
* Description: The official Helm chart repository that provides a curated collection of charts maintained by the Helm project.

Extra:

HELM PRE-REQS
Add s3 Plugin to Helm
# helm plugin install https://github.com/hypnoglow/helm-s3.git

Add s3 to Helm Repo
# helm s3 init s3://<your-bucket-name>/<path-to-charts>
Note:
- S3 is Cloud Based Object Storage
- It uses HTTP for transport
- Offers a Restful API to interface with objects in the bucket. 

Note:
helm s3 init:
* helm s3 init is a command provided by the Helm S3 plugin.
* It is used to initialize an S3 bucket as a Helm chart repository. This means that you can use an S3 bucket to store and serve Helm charts.
* You specify the S3 bucket's path as the repository URL when using this command.
* Example:
# helm s3 init s3://my-helm-bucket/charts

helm repo add:
* helm repo add is used to add a remote Helm chart repository to your Helm client.
* It's typically used for public or private Helm chart repositories hosted on HTTP/HTTPS 
# helm repo add stable https://charts.helm.sh/stable

# helm repo update

PACKAGE HELM CHART
# helm package - -version <name>.v1.0.0 .     (Archiving Helm chart in current directory)

COPY CHART to S3 Bucket assuming AWS CLI is installed. 
# aws s3 cp /path/to/your/helm/chart.tar.gz s3://your-s3-bucket-name/path/to/destination/

DEPLOY CHART straight from Bucket
# helm install my-release s3://<your-bucket-name>/<path-to-charts>/<chart-name>


17.) Have you use Jenkins in multi node setup? If yes, explain how to add a new slave/follower to the master?
Directive: Practice setting up through Terraform.
* Yes I worked in a Jenkins Multi-node Setup. High Availability. 
* Log into Jenkins - Manage Jenkins - Manage Nodes and Clouds - Give Slave Info: (Node IP, PW, Username…”
Gets Register W/ Mater Node

a.) Can we add slaves dynamically?
* Sure, Add an Auto Scaling Group 

18.) Explain how you would block an IAM user from accessing a specific s3 bucket?
* Go into s3… Under Permissions / Bucket Policy…
* Mention the IAM User ARN and DENY Permission

19.) You encounter a docker image with size 2.7GB… Cause for Concern? 
* Depends on what the Application is being used for
* The Bigger it is - results in LONGER BUILD TIME, Download Errors.

A.) If yes, how would you tackle this?
* Use smaller Image (Alpine Image)      /// Light and doesn’t have unnecessary packages installed.
* Remove packages binaries and  remove packages that are NOT required. 

NOTE:
When IAM users or groups are created - they are assigned a unique identifier called  an ARN (Amazon Resource Name),

20.)  What does this IAM POLICY mean? Can you read it.

{ 
“version “ “ “ 2012-10-17”, 
“Statement”: [
     {
        “Sid”: “AllowS3ListRead”,  <——  Organized Unique identifier.  
        “Effect: “Allow:,		<——  Allow the following actions
        “Action”: [
              “s3:GetBucketLocation”,
              “s3:GetAccountPublicAccessBlock”,   <—— These are the actions we’re allowing.
              “s3:ListAccessPoints”


21.) What is PV and PVC in Kubernetes? What role do they play?
*  PV is a cluster wide resource and represents the physical storage (Disk or NAS/External Storage Device) for the cluster.
* PVC is a request for storage by a user or application running in a namespace (project).

22.) When using Terraform to create RDS, how do you save the DB username and password securely?
* Use a Hashicorp Vault to store creds, mention Vault in the terraform Provider Block
* DB’s are  used to retrieve info  from a data sources or query data (Internally/Externally).
* Write Data Block inside Main.tf requesting those secrets.   /// Data Block query/retrieve info from Data Resources. 

23.) What kinds of DB’s have you encountered, supported in the past?
- ETCD is a key-value store DB.
* It stores crucial configuration and state information of the Cluster. 
* Little of RDS/MySql
( Formulate more in-depth experience w/ MySql)

24.) Your security team has insisted that certain packages (Security) related ha to be  present on all the instances for defense project.

How would you come up with a solution for such a design and ensure these packages will be present on all EC2 Instances that shall be used? 
* Use Custom AMI’s as a solution…
* Packer enables you to create identical machine images for multiples platform from a single source template.
* JSON ——> Packer —-> OS Image.

25.) Why do we have 3 different kinds of IAM policy types?
- Managed Policy: Created and maintained by AWS

- Customer Managed Policy: Customer-managed policies are separate standalone policies that you can create, edit, and then attach to 
multiple IAM users, groups, or roles as needed. Will persist after IAM, U or G is deleted. 

- Inline Policy:  Inline policies can be created and are specific to an IAM user, group, or role. They cannot be reused or shared 
with other users, groups, or roles. When U,G or R is deleted so is the inline policy. 

26.) Which build tools are you aware of and what are the build tools used in the project?
Note:
> Build tools are critical for packing the application code and shipping it. 
> Can ship raw code files to servers/clients

Answer:
I have worked on multiple projects where the developers team used Java / NodeJS build tools

Java					NodeJS			Python
Maven, Gradle 			npm				PIP & Pybuilder 

NOTE: PLUGINS NEEDED FOR JENKINS
* Pipeline Plugin: 
* Github Plugin
* Git Plugin: 
* Maven Plugin: 
* Gradle Plugin:
* NodeJs Plugn
* Docker Puglin
* Surefire

1.) 
stage(‘Build’)
	sh 'mvn clean package'
- Maven compiles source code and any test source code. 
‘Sh ‘mvn clean package’
- clean: This removes any previously generated build artifacts.
- package: This rebuilds the project and packages it into its final distribution format.

2.) 
state(‘Test’)
JUNIT is a framework designed for writing and running test on small units of the code (Classes & Methods). 
Maven knows where to find the Junit test classes and main source code “src/test/java, src/main/java”
	sh ‘mvn test’
- Execute Unit Test - Test are written using the framework ‘JUnit’.
- Maven run all the TEST CLASSES
- Reports results to console
If EXIT CODE = 0   /// Application is deployed into 

Use this as part of STC Duties / Use AI to clear this up. Provide detailed scenarios…
Integration Tests: Integration tests come next in the testing hierarchy. These tests focus on ensuring that different parts 
of the application work correctly when integrated or connected. Integration tests verify interactions between components, modules, 
services, or even external systems. They test scenarios where data or control flow moves across various parts of the application.
* Example: In a web application, an integration test might involve testing how the frontend interacts with the backend API and how 
data is passed between them. This could include testing API endpoints, database connections, and the overall behavior of the application 
when these components are combined.


In order to run tests during the build process, it must use plugins like Surefire or Failsafe.


27.) What is ingress and egress?  Where are these terms mostly associated?
* Ingress: Incoming traffic
* Egress: Outgoing traffic
You see this term mostly associated w/ SECURITY GROUPS.

28.) What’s the difference between docker image vs docker layers?
SIDE NOTE: They’re completely different from one another. 
* A Docker Image is the complete package that defines the container
* The layers are the individual, reusable components that make up the image and store the changes.

NOTE:
If you do:
# docker history <image_name>    /// Show all the LAYERS that have been used.  

Example:
FROM rails:onbuild 						—> Use “onbuild” image as a starting point for the build. 
ENV RAILS_ENV dev						—> Setting the “RAILS_ENV” to “DEV”
ENTRYPOINT [“bundle”, “exec”, “logica”]		—> Tells Docker what command to run when Container is Started.

Each line in “Example” is considered a layer —> Creates an DOCKER IMAGE!


29.) What is a bastion host or gateway server? What role do they play?
NOTE: There are many names… Bastion Host, Gateway Server, Jump Box.
* A middle man between a public network and private. Once logged on - I can jump to in server in the private network. 
* From external network - manages access to a Private Network from External network. 
A.) How would stop the flow of traffic into the private network?
* Shut down the Bastion Host.

30.) Auto Scaling group for a project is having issues w/ getting/provisioning new nodes. It’s using complete spot instances. 
        What could be the issue?

* Spot Instances are spare compute capacity offered at a low cost by Amazon. On-Demand Instances.
1.) SPOT PRICE INCREASE /// Then Spot Instances can be taken away from you.
2.) EC2 quota limit. /// Could be a SOFT LIMIT set on the AWS Account. 


DOMAIN: Kubernetes
30.) A pod is trying to access a volume but it gives an access error. We want the pod to have access so what must we do?
Dose PV support the access mode we need. 
* Check the PV / PVC manifest are configured correctly - same “access mode” 
* Check pod configuration —> kubectl describe pod <pod_name>
	* Check for correct PVC Name, Filesystem Permissions, MountPath

31.) How is your K8’s setup done on AWS 
* Were using EKS for Production and Development Services (Why EKS? Flexibility, less maintenance.)
* The cluster builds are provisioned through Terraform.

32.) What are the load balancers in AWS? Briefly explain which ones you have used and their use cases?
* Classic Load Balancers
* Application Load balancer

Howto - CONFIGURE ALB
You config  the listeners on the ALB, create target groups for the listeners. ABL routes the traffic to the appropriate TGs. Assoc. TGs with the ASG,  register instances in the ASG w/ TG's assoc w/ ALB

In summary, the ALB acts as the entry point for incoming traffic and intelligently routes requests to the appropriate target groups associated with ASGs. It monitors the health of instances and ensures that traffic is evenly distributed while maintaining high availability and reliability for your applications hosted in the ASGs.

* Network Load Balancer
* Gateway Load Balancer

(ALBs)
Why do you used these????
We are a micro services shop so we use ALB’s a lot more
Because of their ability to handle microservices with advanced base routing.

                                      +--------------+
                                  |   Internet Gateway |
                                      +--------------+
                                                 |
                                    +------------------------+
                                    |   Gateway Load Balancer  |
                                    |        (GWLB)            |
                                    +------------------------+
                                                 |
                                    +------------------------+
                                    |   Firewall Appliance    |
                                    +------------------------+
							       I
							  VPC
                                                 		|
                                    +------------------------+
                                    |   Application Load      |
                                    |   Balancer (ALB)        |
                                    |                        |
                                    |   - Microservice A     |
                                    |   - Microservice B     |
                                    |   - ...                |
                                    +------------------------+
                                                 |
                                    +------------------------+
                                    |   Auto Scaling Group    |
                                    |   (Microservice A)     |
                                    |                        |
                                    |   - EC2 Instances      |
                                    |   - ...                |
                                    +------------------------+
                                                 |
                                    +------------------------+
                                    |   Auto Scaling Group    |
                                    |   (Microservice B)     |
                                    |                        |
                                    |   - EC2 Instances      |
                                    |   - ...                |
                                    +------------------------+

* Internet: This is where incoming traffic from the internet originates.
* Gateway Load Balancer (GWLB): The GWLB acts as the entry point to your Virtual Private Cloud (VPC) and routes traffic to the firewall appliance for inspection and filtering.
* Firewall Appliance: This is where your security policies and filtering rules are applied to incoming traffic. It ensures that only allowed traffic is forwarded to the Application Load Balancer.
* Application Load Balancer (ALB): The ALB is used to distribute traffic among your microservices based on advanced routing rules (e.g., path-based or host-based routing).
* Microservices (Microservice A, Microservice B, etc.): Behind the ALB, you have multiple microservices hosted in separate Auto Scaling Groups (ASGs). Each ASG contains EC2 instances that serve a specific microservice.
* Auto Scaling Groups: ASGs are responsible for automatically scaling the number of EC2 instances based on demand, ensuring high availability and scalability for your microservices.



Internet
   |
[GWLB]
   |
[Firewall]
   |
[VPC]
   | \
   |  \
   |   \
[ALB]  [Microservices]
   |     |
[Target Groups]

ALB Additional Interview questions.


* Question: Explain the difference between path-based routing and host-based routing in ALB. Answer: Path-based 
routing routes traffic based on the URL path Host-based routing routes traffic based on the hostname (subdomain) in the URL.

* Question: How can you implement cross-zone load balancing with ALB, and what are the benefits? Answer: Cross-zone load 
balancing is enabled by default. It evenly distributes traffic across instances in all availability zones. 

* Question: What is the difference between ALB and NLB in terms of layer and use cases? Answer: ALB operates at 
Layer 7 (application layer) - ideal for routing traffic based on content. 
Network Load Balancer (NLB) operates at Layer 4 (transport layer) and is designed for high-performance, low-latency 
applications like TCP/UDP load balancing.


33.) What is Sonarqube, have you used it?
It’s an automatic code review tool - detects bugs and vulnerabilities in the code. No I haven’t used it. 

Dev —> Checks the code into Github
	       Excuses a PR
	       Sonarq Checks the code for vulnerabilities and bugs
               Sends a report back - Pass or Fail
	       Peer review team steps in and reviews the code.
		+1 
	       Sonarq merges the code
		Jenkins job build the app

.34) How do you handle incidents in your team?
We monitor the environment with  Prometheus, Grafana. 

Application expose the —> Metrics —> Prometheus pulls Metrics —> Alert rules —> Triggers ONCALL (Email Slack & Phone)
* The App exposes the metrics
* We have Prometheus configured to pull the metric in.
* Based metric values, Alert rules will trigger an alert to the On-Call Engineer. /// (Phone Call / Slack)
* Engineer will come online, try to solve it or escalate to a higher level of support.

- Once Incident fix
- RCA Root Cause Analysis is performed
- Mitigation report is created with Solution


35.) Explain CMD and ENTRYPOINT. Are they the same or different. 
* They both specify what command should be executed on container startup. 
* They both can run the same commands, use the same shells, when used both together - you can pass values btw ea other.
* CMD is more of a default and you can change the behavior outside the container. You can’t do that  w/ ENTRYPOINT.
* Entry point is more of a cemented main process. Once you set it that’s it. 

36.) Multiple EDC2 instances in ASG is getting terminated and this is causing downtime on the application. EC2 pricing, quota all look good. How would you start debugging this issue. 

CHECK THE FOLLOWING:
* CPU
	# top 		/// Find the Offending process 
* DISK
	# df -h 
	# du -sh /var/log /tmp	/// Clear out some log space.
* MEMORY
	# free -mt		/// If SWAP = 0 That’s a problem too. App is trying to utilize all the mem. 

ASG —> EC2 —> Application —> unhealthy (ASG Terminates EC2) 

37.) What does terraform state lock really mean? Do we have a practical use of it?
When you execute 	///		# terraform apply
* Prevents concurrent access to the same Terraform state file - avoiding conflict and data corruption to the TF  infrastructure.

38.) Currently you have a Jenkins job, but we don’t have any kind of notifications on job success or failures. We use soak for our internal communications.
How would you enable a notification service in this case?

* Install the Email and Notification Plugin for SLACK.
* Configure with Authentication Token and Slack Webhook.

39.) In a Ansible playbook, We have a section which could fail on certain nodes. But we don’t want the playbook to stop or exit b/c of this.
Is it possible to ignore this if this part of the playbook fails. 

* Yes, by using the “ignore_errors sub  argument”
- name: Do not count this as a failure
	ansible.builtin.command: /bin/false
		ignore_errors:  yes

40.) If you were to make a change for a commit done 30 days ago with respect to a machine type used?
How would you go about doing this change.
Would you directly change it?

* No, I wouldn’t directly change it w/o knowing to the history behind it. 
- Got to repo (Github UI) and check GIT BLAME for history —> and then possibly make those changes. 
a.) # git clone on the Repo
b.) Create a new branch
c.) Apply the changes

41.) Create a script that will push certain logs to S3 automatically.
Explain only high level design which is enough. Share what all steps you would do to achieve this above script. 

Note: You can used Python “Time” module or put Python script in Linux Cron Job.

import boto3
import os
import glob
import time

# Initialize the S3 client
s3 = boto3.client('s3')

# Specify the local directory where your log files are located
log_directory = '/path/to/log/files/'

# Specify the S3 bucket name and prefix (folder) where you want to store the logs
bucket_name = 'your-s3-bucket-name'
s3_prefix = 'logs/'

# Specify the file extensions of the logs you want to upload
log_extensions = ['.log', '.txt']

# Calculate the time until the next scheduled execution (in seconds)
# For example, to run the script at 2:00 PM, set the desired_hour to 14 (24-hour format)
desired_hour = 14  # Adjust this to your desired hour
current_hour = time.localtime().tm_hour
seconds_until_execution = (desired_hour - current_hour) * 3600  # 3600 seconds in an hour

# Sleep until the desired execution time
if seconds_until_execution > 0:
    print(f"Sleeping for {seconds_until_execution} seconds until {desired_hour}:00 PM...")
    time.sleep(seconds_until_execution)

# Get a list of log files matching the specified extensions in the local directory
log_files = [f for f in glob.glob(os.path.join(log_directory, '*')) if os.path.splitext(f)[1] in log_extensions]

# Iterate through the log files and upload them to S3
for log_file in log_files:
    # Extract the file name without the path
    log_filename = os.path.basename(log_file)
    
    # Construct the S3 object key (path in the bucket)
    s3_object_key = s3_prefix + log_filename
    
    # Upload the log file to S3
    s3.upload_file(log_file, bucket_name, s3_object_key)

print("Log files uploaded to S3 successfully.")


42.) Explain in simple terms what is a package in Python?
* Python package is  a collection of Python modules in a directory hierarchy starting with a special file __init__.py
- Modules have functions
* A Python Module = It a file with Python code, variables and data. 
* “OS” is a package, ea module has a particular usecase and multiple functions available.

IMPORTANT QUESTION WILL BE ASKE FIRST….
43.) Tell me about yourself and what your day to day task being a DevOps engineer?

43/A.) What Is DevOps
It’s a methodology, a culture. A collaboration between Developers and IT operation to stream line code through Automated testing and deployment. Bottom line - Accelerate Software Delivery. 

My day starts at 9am 
- Check my slack messages
- Update my Jira tickets 
- We have a daily standup (Discuss what I’ve done, what I’m working on today
- After, I’ll got work on my Jira tickets and latest request.
* Code Reviews
* AWS request 
* Making Operations changes
* Work with devs on: Design discussion and request to setup integrations test in the Kubernetes environment. 
* Maven projects

Domain: K8s
44.) We want to launch certain pods only on specific nodes. These nodes should explicitly use only the pods. Is it possible to control the deployment of pods in such a given scenario?

* You use “NODE TAINTS”
# kubectl taint nodes <node-name> <taint-key>=<taint-value>:NoSchedule
Pod w/ a matching toleration will deploy on the tainted node.
Pod w/o will be repelled from tainted node.


apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
  tolerations:
  - key: <taint-key>
    operator: Equal
    value: <taint-value>
    effect: NoSchedule

45.) Let’s say you have 2 servers with different ports and username then how can ansible handle different machines for applying and running the same runbook?

* Create a separate inventory group for each server w/ specific connection details. Then specify the target those target groups when running the playbook.

Example:

Suppose you have two servers:

Server 1:

IP Address: 192.168.1.100
SSH Port: 2222
Username: user1

Server 2:

IP Address: 192.168.1.101
SSH Port: 22
Username: user2


Create an Ansible inventory file (e.g., inventory.ini) with the following content:
————————————————————————
[web_servers]
192.168.1.100:2222 ansible_ssh_user=user1
192.168.1.101 ansible_ssh_user=user2
———————————————————————

Now, you can run your Ansible playbook against these servers. For example, if your playbook is named my_playbook.yml, you can run it like this:

# ansible-playbook -i inventory.ini web_servers my_playbook.yml



46.) What is the difference between a Web Server and Application Server?

* Web servers serve web content, while application server on the backend runs web app logic.

47.) How can we handle secrets in ansible? Let’s say we have a playbook which needs to login to premises server with login name and password

* # echo “data to be  encrypted’> encrypt_file.txt
* # ansible-vault encrypt encrypt_file.txt
* Or integrate it with Hashi Corp Vault

48.) What is a Blue / Green deployment?
It’s a software release strategy where you have 2 identical environments. Blue representing the current running  prod env and green contains the updated release. You switch over the traffic to Green for testing and can easily be rolled back. 


Blue —> Running  v1.0
[ Network Switch is pointing to BLUE ]   (Routing Control by ROUTE53 or F5)

- New deployment =  1.01
- Deploy on “Green”
Green —>  now Running v1.01

- Route Traffic from Blue to Green
[ Network Switch is pointing to GREEN ]	(Routing Control by ROUTE53 or F5)

- Now we MONITOR the GREEN Environment
- Any Issue: Change network config to roll back to BLUE .


49.) What does a Mult-Branch Pipeline project intel?

You need different branches in your git repo. W/ separate Jenkins Files
- Master - JenkinsFile
- Staging - JenkinsFIle
- Dev - JenkinsFile

.50) What is a Market Place AMI
It’s where you can find external software that can be used w/o signing a specific contract. 
It’s basically how 3rd party companies earn money
* You spin up a EC2 instance - Cost paid to AWS
* You use say RedHat AMI with that EC2 - Cost is paid to Redhat.

51.) A dev team wishes to go with a NoSQL DB, but what is this NoSQL DB and do we have something in AWS that can be used as a NoSQL DB?

Store large volumes of un-structured data, no fix schemas. 
* AWS DynamoDB (key/value Document Store), DocumentDB

52.) We have an on-premises linux server which needs some monitoring enable. Being a DevOps engineer only you have access to this server. How would you setup basic monitoring script on it?

Write a Python script w/…
* Import psutil                 // This python module can monitor any kind of server resources. 
* Exist in Pipit package
* # pip install psutil
* Cpu, load, memory Disk usage, Disk I/O

53.) Why is logging important? What is centalised logging and what tools help to achieve centralized logging?

* Logging can help you debug and fix issue pertaining to your application. 
* Centralize logging you can dump to: Splunk, S3,  CloudWatch, Elastic Search, DataDog(Log Aggregator)

54.) What is a side car container?
* Supports the main application container’s operations through monitoring and logging.
* It’s a small, supplementary container that runs alongside the main application container. 
NOTE:
Multiple container running in a POD can be destroyed at any time  so their logging is directed
To the SIDE CAR CONTAINER. 

55.) Terraform apply is failing. The process is in a hung state. Now if you cancel this it might result in corrupt terraform state.
How would you debug this issue? 

* I’d save the remote statefile first
- Connect to a new Terminal 
- terraform state pull     ///    This pulls remote statefile to your local desktop filesystem like a backup an avoiding corruption. 
- Kill the process “terraform apply” and run it again. 
- If it doesn’t work then I’ll just replace the current statefile w/ my backup. 

DOMAIN: Github
56.) What is branch protection in GitHub?

Discussion:
This is the proper way to merge code…
Working branch —> You make changes —> Open a PR —> Get a review —> Merged Code to Master/Main Branch
- If there are no protections in place then a dev can just BYPASS (PR & Review) and merge the code. This is incorrect!

* You must enable “Branch Protection Rules”
- Goto Master Repo, Click Settings, Click Branches, Click Add branch protection rules, Check “Require Pull Request B4 Merging” (Requiring x-number of approvals.)








