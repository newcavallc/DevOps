

** PROMETHEUS ARCHITECTURE EXPLAINED **

Targets (Applications and Services):
  * These are the applications, services, or devices you want to monitor.
  * Targets expose metrics and are the sources of monitoring data.

Prometheus Server:
  * The central component that collects, stores, and queries metrics.
  * Responsible for scraping (pulling) data from targets at specified intervals.

Exporters:
  * Optional components that help expose metrics from systems and services that don't natively support Prometheus.
  * Exporters act as bridges, converting other data formats into Prometheus-readable metrics.
  * Examples include Node Exporter, Blackbox Exporter, and more.

Service Discovery:
  * Part of the Prometheus server that dynamically discovers and monitors targets.
  * Supports various service discovery methods, including Kubernetes service discovery, DNS, file-based discovery, and more.
  * Data Storage (Time-Series Database, TSDB):

Prometheus stores collected metrics in its built-in time-series database.
  * Metrics are stored as time-series data, making it easy to query historical data.
  * The local storage can be configured to retain data for a defined retention period.

Alert Manager:
  * An independent component responsible for handling and managing alerts generated by Prometheus.
  * Provides features for deduplication, silencing, grouping, and routing alerts to appropriate recipients.

//////

The Alert Manager can alert you by:
Receiving Alerts: 
It receives alerts from the Prometheus server.

Processing and Grouping Alerts: 
The Alert Manager processes incoming alerts, groups related alerts, and deduplicates them.

Applying Notification Routing: 
It applies notification routing rules to determine which receivers (e.g., email, chat, webhook) should receive alerts.

Sending Notifications: 
It sends notifications to the configured receivers, such as email, chat, or other communication channels.

In summary, the Alert Manager acts as a central component for processing and routing alerts generated 
by Prometheus, ensuring that the right people or systems receive timely notifications about issues and incidents.

\\\\\\

Query Language and Web UI:
  * Users can interact with Prometheus through its query language (PromQL).
  * Prometheus provides a web-based user interface for querying and visualization.

Grafana (Optional):
  * An external visualization and dashboard tool that can be integrated with Prometheus.
  * Allows for creating custom dashboards and advanced visualization of Prometheus data.

Push Gateway (Optional):
  * An optional component that allows short-lived jobs to push their metrics to Prometheus.
  * Useful for batch jobs or jobs that can't be scraped by Prometheus.

-- INSTALL HELM --
Step 1: Install Helm
Helm Install Guide:   https://helm.sh/docs/intro/install/

-- HELM REPO --
Step 2: Add Prometheus Helm Repository
# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
# helm repo update

COMMAND BREAK DOWN
* helm repo add         ///    Adds the Prometheus community repository to Helm.
* helm repo update     ///     Updates the local Helm repository cache.

-- INSTALL PROMETHEUS --
Step 3: Install Prometheus with Helm
Save the values in a "values.yaml" file to customize the installation.

# values.yaml

server:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: "nginx"  # Modify if you use a different ingress controller
    hosts:
      - prometheus.example.com  # Replace with your domain
    pathType: Prefix
    path: /

-- we configure Prometheus to be accessible via an Ingress with a custom domain. 
   Adjust the hosts value to your domain.

Run the Helm installation command:
# helm install prometheus -n prometheus -f values.yaml prometheus-community/prometheus

COMMAND BREAK DOWN
* "helm install"          ///   Installs" the Prometheus Helm chart.
* "prometheus"            ///   Is the release name for this installation.
* "-n prometheus"         ///   Specifies namespace where Prometheus will be installed. ENSURE THAT NAMESPACE EXIST.
* "-f values.yaml"        ///   Specifies the values file to customize the installation.
* "prometheus-community/prometheus"    ///   Is the Helm chart to install.

Step 4: Verify Installation
Check the installation status and ensure that Prometheus pods are running:

# kubectl get pods -n prometheus
You should see Prometheus pods running. If the status is "Running," the installation was successful.

Step 5: Access Prometheus WEB UI
Open your browser and navigate to http://prometheus.example.com/.
REMEMBER: We specified this domain earlier in our values.yaml file. 

CONGRATS!!!
We installed Prometheus in your Kubernetes cluster using Helm and customized the installation with specific configuration values. 
You can further configure and integrate Prometheus as needed for your monitoring requirements.


-- LET'S CONFIGURE PROMETHEUS SERVER W/ SCRAPE TARGETS --
STEP 1: Create/Edit the values-custom.yaml file and specify the scrape target(s) for Prometheus.

i,e.
# custom-values.yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'my-app'
    kubernetes_sd_configs:
      - role: endpoints
    relabel_configs:
      - source_labels: [app]         
        action: keep                       
        regex: my-app                      
      - source_labels: [container_port] <---      We use yet another sourcel label for same job... Therefore we narrow down 
                                                  the selection of pods for scraping even further. 
        action: keep
        regex: "8080"

NOTICE ABOVE...
- source_labels: [app]                ///       Therefore if your Deployment App indicates the following:
                                                labels:
                                                  app: my-app  # Applying the "app" label to the pod template

                                                Prometheus will filter pods to monitor based on the "[app]" lable and "my-app" value.
                                                Prometheus will only scrape metrics from pods that match this label criteria.

< CONFIG BREAK DOWN >

GLOBAL SECTION:

global:: 
This section contains global settings for Prometheus.

scrape_interval: 15s: 
This line sets the global scrape interval, which is the frequency at which Prometheus scrapes metrics from its targets. 
In this case, it's set to every 15 seconds.

Scrape Configurations Section:

scrape_configs:: 
This section defines the scrape configurations, which specify the targets and settings for scraping metrics.

Job for Prometheus:

job_name: 'prometheus': 
This is a scrape configuration named 'prometheus.' It's used for scraping metrics from the Prometheus server itself.

static_configs:: 
Specifies static (fixed) scrape targets.

targets: ['localhost:9090']: 
This line specifies that Prometheus should scrape metrics from 'localhost' on port 9090, 
which is how Prometheus collects its own metrics.

Job for Your Application ('my-app'):

job_name: 'my-app': This is a scrape configuration named 'my-app,' representing your application's metrics.

kubernetes_sd_configs:: 
This section configures Kubernetes service discovery for finding scrape targets.

role: endpoints: 
Specifies the role for Kubernetes service discovery, focusing on endpoints (pods).

relabel_configs:: 
Defines relabeling configurations to filter or modify discovered targets.

source_labels: [app]: 
Source label for filtering.

action: keep: 
The action specifies to keep the target if conditions are met.

regex: my-app: 
Specifies the regex pattern. This line filters pods with the 'app' label set to 'my-app.'
Another relabeling rule filters pods with the 'container_port' label set to "8080."


PROMETHEUS SCRAPES ITSELF: 

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']


IT ALSO SCRAPE FOR:

Prometheus Uptime: 
Information about how long the Prometheus server has been running.

Resource Usage: 
Metrics related to CPU and memory usage by the Prometheus process.

Scrape and Alerting Information: 
Metrics related to the success and latency of scraping other targets 
(including your application) and alerting rule evaluations.

TSDB (Time-Series Database) Metrics: 
Data about the storage and management of time-series data, including information on 
disk space usage, retention policies, and more.

HTTP Request Statistics: 
Metrics related to HTTP requests to the Prometheus server, such as request counts, 
response times, and error rates.

Prometheus Configuration: 
Information about the configuration of Prometheus, including scrape targets and 
alerting rules.


- So how does the Prometheus server know where to find the pods?
<> Kubernetes Service Discovery: When Prometheus starts, it queries the Kubernetes API server for 
                                 the list of pods that match the criteria specified in the service discovery configuration. 
                                 This criteria often includes Kubernetes labels and annotations.

<> Dynamic Target Discovery: Prometheus dynamically discovers the pods that match the criteria and adds them as scrape targets. 
                             This process is automated and does not require manual intervention to update scrape targets.

<> Scraping: Prometheus regularly scrapes metrics from the discovered pods according to the scrape interval defined in the configuration.




Step 2: Upgrade Prometheus
To apply changes, we use 'helm upgrade' command with the release name and the path to the Helm chart:

# helm upgrade prometheus -n prometheus -f custom-values.yaml prometheus-community/prometheus

COMMAND BREAK DOWN
* 'prometheus'                       ///         is the release name.
* '-n prometheus'                   ///          specifies the namespace.
* '-f custom-values.yaml'           ///          specifies custom values file that includes the updated scrape target configuration.
* 'prometheus-community/prometheus' ///          is the Helm chart.

This command will upgrade the Prometheus deployment with the new configuration, including the updated scrape targets.

Step 3: Verify Changes
Verify changes by checking Prometheus pods to ensure they're running/collecting 
data from the updated scrape targets:

# kubectl get pods -n prometheus
Your Prometheus pods should show that they are running and ready. 
The changes to the scrape targets will take effect after the upgrade.


-- Let's Arm the Application --


STEP 1: Update pom.xml: Add Prometheus Java client library as a dependency. 
        Here's an example of how to do it:

<dependencies>
    <!-- Other dependencies -->
    <dependency>
        <groupId>io.prometheus</groupId>
        <artifactId>simpleclient</artifactId>
        <version>0.12.0</version>
    </dependency>
    <dependency>
        <groupId>io.prometheus</groupId>
        <artifactId>simpleclient_hotspot</artifactId>
        <version>0.12.0</version>
    </dependency>
    <!-- Additional Prometheus client libraries as needed -->
</dependencies>

This i.e, includes two common Prometheus client libraries for Java:
      - simpleclient 
      - Simpleclient_hotspot
        NOTE: You may need other libraries depending on your specific use case.


STEP 2: Instrument Code: 
You kneed to instrument your Java code to expose metrics using the following libraries. 

import io.prometheus.client.Counter;                <-------
import io.prometheus.client.exporter.HTTPServer;     <-------

public class Main {
    public static void main(String[] args) throws Exception {
        HTTPServer server = new HTTPServer(8080);

        Counter requests = Counter.build()
            .name("my_app_requests_total")
            .help("Total HTTP requests handled")
            .register();

        requests.inc();

        Thread.currentThread().join();
    }
}


-- WHAT THIS LOOKS LIKE FOR KUBERNETES DEPLOYMENT --

Remember: We've already exposed the metric in Step 1: 
Step 2: Create Kubernetes Deployment and Service.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app          <---- Prometheus will filter based on the "app:" label
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app
          image: your-java-app-image:tag
          ports:
            - containerPort: 8080  # Expose the port your application is running on


---

NOTE: The 'Service' facilitates network communication to and from the POD. 

apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80  # Exposed port
      targetPort: 8080  # Match the container port



< DEPLOYMENT BREAK DOWN >

apiVersion: apps/v1: 
This line specifies the API version for the Kubernetes object being created. 
In this case, it's using the apps/v1 API version, which is commonly used for Deployments.

kind: Deployment: 
This line specifies that you are creating a Deployment object. 
A Deployment is a Kubernetes resource that defines a declarative way to manage applications.

metadata:: 
This section contains metadata about the Deployment, such as its name and labels.

name: my-app: 
This line specifies the name of the Deployment. In this example, it's named "my-app."

labels:: 
Labels are key-value pairs that can be attached to Kubernetes resources. 
Here, a label with the key "app" is set to the value "my-app." This label can be used for grouping and selecting resources.

spec:: 
The spec section defines the desired state of the Deployment.

replicas: 3: 
This line specifies that you want three replicas (copies) of your application pods 
to be running at all times. If a pod goes down, the Deployment will ensure that it's replaced.

template:: 
Inside the template section, you define the Pod template for the Deployment. 
This template is used to create individual pods that are part of the Deployment.

metadata:: 
Similar to the outer metadata section, this metadata section contains labels for the pods 
created from the template.

labels:: 
These labels are associated with the pods created from the template. 
In this case, the pods will also have the label "app: my-app."

spec:: 
Inside the template's spec section, you specify the configuration for the containers running in the pods.

containers:: 
This is a list of containers running in the pod. In this example, there's only one container named "my-app."

name: my-app: 
This line specifies the name of the container, which is set to "my-app."

image: 
your-java-app-image:tag: Here, you specify the Docker image for the container. 
Replace "your-java-app-image:tag" with the actual image name and tag for your Java application.

ports:: 
This section defines the ports that the container exposes.

- containerPort: 8080: It specifies that the container is listening on port 8080. 
This port is where your Java application inside the container is running and exposing its service.



< SERVICE BREAK DOWN >

apiVersion: v1: 
Specifies the API version for the Kubernetes Service.

kind: 
Service: Indicates that you are creating a Service resource in Kubernetes.

metadata:: 
This section contains metadata about the Service, such as its name.

name: my-app: 
Specifies the name of the Service, which is set to "my-app."

spec:: 
Defines the specifications for the Service.

selector:: 
This section defines a set of labels used to select the pods that the Service will route traffic to.

app: my-app: 
The selector specifies that the Service will route traffic to pods with the label "app: my-app," 
matching the labels of the pods created by the Deployment.

ports:: 
This section specifies the ports that the Service will listen on.

- protocol: TCP: Indicates that the Service is using the TCP protocol.

port: 80: 
Specifies the port on which the Service will listen. This is the port through which external 
clients can access the Service.

targetPort: 8080: 
Specifies the target port to which traffic will be routed within the pods. 
This matches the container port of the pods created by the Deployment. In this case, it routes 
traffic to port 8080 within the pods.

-- I THINK WE'RE DONE FOR NOW --




-- GRAFANA INSTALL PROCEDURES --

Step 1: Prerequisites

Before you begin, make sure you have the following prerequisites in place:

A running Kubernetes cluster.
Helm installed on your local machine and initialized on your cluster.
Access to your cluster, including permissions to create and manage resources.

Step 2: Add the Grafana Helm Repository

Add the official Grafana Helm repository to Helm so that you can install Grafana from there. Run this command:
# helm repo add grafana https://grafana.github.io/helm-charts

Step 3: Update the Helm Repositories

Update the Helm repositories to ensure you have the latest information about available charts:
# helm repo update

Step 4: Create a Values File (Optional)

You can customize Grafana's configuration by creating a values.yaml file with your desired settings. For example:
# values.yaml
adminUser: your-admin-username
adminPassword: your-admin-password

Step 5: Install Grafana using Helm

Use Helm to install Grafana with your custom values (if you created a values.yaml file) or with the default values:
# helm install grafana grafana/grafana -f values.yaml

Step 6: Verify the Installation

Check if the Grafana installation was successful by running:
# kubectl get pods

You should see a Grafana pod in the output.

Step 7: Access the Grafana Dashboard

To access the Grafana dashboard, you'll typically need to create a service or an ingress resource. 
Here's an example of how to expose Grafana using a ClusterIP service:

# grafana-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
spec:
  selector:
    app.kubernetes.io/name: grafana
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000

Apply this service definition:
# kubectl apply -f grafana-service.yaml

Step 8: Access the Grafana Web Interface

Find the IP or hostname of your Grafana service or ingress, and access it in your web browser. 
The default username and password are typically "admin" and "admin." You'll be prompted to change 
the password upon the first login.

That's it! You've successfully installed Grafana in your Kubernetes cluster using Helm and can now 
use it to create dashboards and visualizations for your monitoring data.


OPTION STEP...
EXPOSE GRAFANA USING AN INGRES CONTROLLER.

Step 1: Create an Ingress Resource

Create an Ingress resource definition for Grafana. Save this configuration in a file, for example, grafana-ingress.yaml:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: grafana.example.com  # Replace with your domain or host
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 80


In this example, we use the NGINX Ingress Controller with the rewrite-target annotation. 
Replace grafana.example.com with your actual domain or host.

Step 2: Apply the Ingress Configuration

Apply the Ingress resource configuration to your cluster:
# kubectl apply -f grafana-ingress.yaml

Step 3: Verify the Ingress

Check the status of the Ingress to ensure it's running and has the desired configuration:
# kubectl get ingress

You should see the Grafana Ingress with your specified host and path.

Step 4: DNS Configuration (if required)

Make sure that the DNS for the host you specified (e.g., grafana.example.com) is correctly 
configured to point to your cluster's Ingress controller IP or load balancer. 
This step is crucial for external access.

Step 5: Access Grafana

Once the Ingress is configured and DNS is set up, you can access Grafana using your specified host or 
domain in a web browser. For example, if you configured it as grafana.example.com, access Grafana 
by going to http://grafana.example.com.

You should now have Grafana accessible via an Ingress controller, allowing you to access the 
Grafana dashboard securely over the internet or your network using a custom domain or host.


-- CREATE GRAFANA DASHBOARD --

Step 1: Access the Grafana Dashboard

Open a web browser and navigate to the Grafana web interface. The URL should be the one configured during Grafana installation, often http://<your-grafana-host>.
Log in using your Grafana credentials.

Step 2: Create a New Dashboard

In the Grafana interface, 
- click on the "Create" icon in the sidebar.
- Click on Dashboard
- Click on the "Add new panel" button            /// to start creating a panel for your application's metrics.

Step 3: Define Data Source

- Click on the panel title, then click "Edit."

- In the panel settings, go to the "Query" section.

- Choose the appropriate data source (e.g., Prometheus) from the drop-down menu.

- Write a PromQL query to fetch the metrics related to your application on the specific port (e.g., HTTP). 
  For example, you might use a query like:

  http_requests_total{job="my-app", port="80"}
  This query retrieves the total HTTP requests metric for a job named "my-app" running on port 80.

Step 4: Configure Visualization

- In the "Visualization" section of the panel settings, select the type of visualization that best represents your metric data (e.g., time series graph, singlestat, etc.).
  Customize the visualization settings to suit your preferences, such as axes, labels, and colors.

Step 5: Save and Name the Panel

- After configuring the panel, click "Apply" to save your changes.
  Give your panel a meaningful title that describes the metric you are monitoring, such as "HTTP Requests."

Step 6: Create Additional Panels (Optional)

- You can add multiple panels to the dashboard to monitor various aspects of your application, 
  such as response times, error rates, or any other relevant metrics. Repeat the steps above for each panel.

Step 7: Save the Dashboard

- Click "Save" in the top right corner of the Grafana interface.
- Enter a name for your dashboard and choose the folder where you want to save it.
- Optionally, provide a description for the dashboard.

Step 8: Access and Share the Dashboard

- You can now access your dashboard at any time by navigating to the "Home" menu in Grafana 
  and selecting the folder where you saved the dashboard.

 - You can also share your dashboard with others by generating a shareable link or by exporting 
   it as a JSON file that can be imported into other Grafana instances.

By following these steps, you can create a Grafana dashboard that monitors an application on a 
specific port, allowing you to visualize and analyze its metrics.







